# Deep Learning & Machine Learning Research Paper Collection

## ğŸ“Œ Overview
This repository is a **comprehensive collection of influential research papers** in **Deep Learning (DL), Machine Learning (ML), Artificial Intelligence (AI), Generative AI (GenAI), CUDA/Triton**, and other related fields. The goal is to provide a structured approach to understanding the **evolution, core concepts, and practical implementations** of these fields.

## âš ï¸ Disclaimer
> This is a **personal learning project**. The implementations and notes may contain errors or simplifications. Use with caution and always refer to the original papers.

## ğŸ¯ Project Goals
- Implement **all** these research papers **from scratch**.
- Provide **scratch implementations** to aid learning and understanding.
- Build a **structured, categorized, and well-maintained repository** for reference.

## ğŸŒŸ Inspiration
Inspired by [@saurabhaloneai](https://github.com/saurabhaloneai/History-of-Deep-Learning.git) and expanded with additional research papers and implementations.

## ğŸ“‚ Repository Structure
```
â”œâ”€â”€ Foundational Deep Neural Networks
â”œâ”€â”€ Optimization & Regularization
â”œâ”€â”€ Sequence Modeling
â”œâ”€â”€ Language Modeling
â”œâ”€â”€ Open Source LLMs & Implementation
â”œâ”€â”€ Architecture Innovations
â”œâ”€â”€ Training Methodologies
â”œâ”€â”€ Image Generative Modeling
â”œâ”€â”€ Deep Reinforcement Learning
â”œâ”€â”€ General Machine Learning Papers
â”œâ”€â”€ CUDA & Triton Optimization Papers
â”œâ”€â”€ Generative AI (GenAI)
â”œâ”€â”€ Scaling & Model Optimization
â”œâ”€â”€ Reasoning & Capabilities
â”œâ”€â”€ Inference & Efficiency Techniques
â”œâ”€â”€ Fine-tuning & Adaptation
â”œâ”€â”€ Graph Neural Networks
â””â”€â”€ Self-Supervised and Few-Shot Learning

```

---

## ğŸ“š Research Papers Collection

### 1ï¸âƒ£ Foundational Deep Neural Networks
- [ ] **Learning Internal Representations by Error Propagation (1987)** [ğŸ“„ Paper](https://arxiv.org/abs/___) [ğŸ’» Repo]()
- [ ] **Learning Representations by Backpropagating Errors (1986)** - Rumelhart et al. [ğŸ“„ Paper](https://www.nature.com/articles/323533a0) [ğŸ’» Repo]()
- [ ] **Backpropagation Applied to Handwritten Zip Code Recognition (1989)** [ğŸ“„ Paper](https://ieeexplore.ieee.org/document/6795724) [ğŸ’» Repo]()
- [ ] **Gradient-Based Learning Applied to Document Recognition (1998)** - Yann LeCun et al. [ğŸ“„ Paper](https://ieeexplore.ieee.org/document/726791) [ğŸ’» Repo]()
- [ ] **ImageNet Classification with Deep Convolutional Networks (2012)** - AlexNet [ğŸ“„ Paper](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) [ğŸ’» Repo]()
- [ ] **Convolutional Networks for Biomedical Image Segmentation (2015)** - U-Net [ğŸ“„ Paper](https://arxiv.org/abs/1505.04597) [ğŸ’» Repo]()
- [ ] **VGGNet (2014)** - Very Deep Convolutional Networks for Large-Scale Image Recognition [ğŸ“„ Paper](https://arxiv.org/abs/1409.1556) [ğŸ’» Repo]()

### 2ï¸âƒ£ Optimization & Regularization Techniques
- [ ] **A Simple Weight Decay Can Improve Generalization (1991)** [ğŸ“„ Paper](https://papers.nips.cc/paper/1991/hash/8eefcfdf5990e441f0fb6f3fad709e21-Abstract.html) [ğŸ’» Repo]()
- [ ] **Deep Sparse Rectified Neural Networks (2011)** - ReLU [ğŸ“„ Paper](https://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf) [ğŸ’» Repo]()
- [ ] **Deep Residual Learning for Image Recognition (2015)** - ResNet [ğŸ“„ Paper](https://arxiv.org/abs/1512.03385) [ğŸ’» Repo]()
- [ ] **Dropout (2014)**: Preventing Neural Networks from Overfitting [ğŸ“„ Paper](https://jmlr.org/papers/v15/srivastava14a.html) [ğŸ’» Repo]()
- [ ] **Batch Normalization (2015)**: Accelerating Deep Network Training [ğŸ“„ Paper](https://arxiv.org/abs/1502.03167) [ğŸ’» Repo]()
- [ ] **Layer Normalization (2016)** [ğŸ“„ Paper](https://arxiv.org/abs/1607.06450) [ğŸ’» Repo]()
- [ ] **Gaussian Error Linear Units (2016)** - GELU [ğŸ“„ Paper](https://arxiv.org/abs/1606.08415) [ğŸ’» Repo]()
- [ ] **Adam: A Method for Stochastic Optimization (2014)** [ğŸ“„ Paper](https://arxiv.org/abs/1412.6980) [ğŸ’» Repo]()
- [ ] **Transformers without Normalization (2024)** - Yann LeCun's Dynamic Tanh (DyT) [ğŸ“„ Paper](https://arxiv.org/abs/2503.10622) [ğŸ’» Repo]()

### 3ï¸âƒ£ Sequence Modeling
- [ ] **Continually Running Fully Recurrent Neural Networks (1989)** - RNN [ğŸ“„ Paper](https://papers.nips.cc/paper/1988/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf) [ğŸ’» Repo]()
- [ ] **Long-Short Term Memory (1997)** - LSTM [ğŸ“„ Paper](https://www.bioinf.jku.at/publications/older/2604.pdf) [ğŸ’» Repo]()
- [ ] **Gated Recurrent Unit (2014)** - GRU [ğŸ“„ Paper](https://arxiv.org/abs/1412.3555) [ğŸ’» Repo]()
- [ ] **Sequence to Sequence Learning (2014)** - Seq2Seq [ğŸ“„ Paper](https://arxiv.org/abs/1409.3215) [ğŸ’» Repo]()
- [ ] **Neural Machine Translation with Alignment (2014)** - Attention [ğŸ“„ Paper](https://arxiv.org/abs/1409.0473) [ğŸ’» Repo]()
- [ ] **Sparsely-Gated Neural Networks (2017)** - Mixture of Experts [ğŸ“„ Paper](https://arxiv.org/abs/1701.06538) [ğŸ’» Repo]()

### 4ï¸âƒ£ Language Modeling
- [ ] **Attention Is All You Need (2017)** - Transformer [ğŸ“„ Paper](https://arxiv.org/abs/1706.03762) [ğŸ’» Repo]()
- [ ] **BERT: Bidirectional Transformers for Language Understanding (2018)** [ğŸ“„ Paper](https://arxiv.org/abs/1810.04805) [ğŸ’» Repo]()
- [ ] **GPT-1 (2018)**: Improving Language Understanding [ğŸ“„ Paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) [ğŸ’» Repo]()
- [ ] **GPT-2 (2019)**: Language Models are Unsupervised Multitask Learners [ğŸ“„ Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) [ğŸ’» Repo]()
- [ ] **T5: Unified Text-to-Text Transformer (2019)** [ğŸ“„ Paper](https://arxiv.org/abs/1910.10683) [ğŸ’» Repo]()
- [ ] **Vision Transformer (2020)**: Image Recognition with Transformers [ğŸ“„ Paper](https://arxiv.org/abs/2010.11929) [ğŸ’» Repo]()
- [ ] Llama (2023): Open Foundation Models [ğŸ“„ Paper](https://arxiv.org/abs/2302.13971) [ğŸ’» Repo]()
- [ ] Llama 2 (2023): Open Foundation Models [ğŸ“„ Paper](https://arxiv.org/abs/2307.09288) [ğŸ’» Repo]()
- [ ] Llama 3 (2024): Open Foundation Models [ğŸ“„ Paper](https://arxiv.org/abs/2404.08735) [ğŸ’» Repo]()
- [ ] Mistral (2023): Efficient Transformer Architecture [ğŸ“„ Paper](https://arxiv.org/abs/2310.06825) [ğŸ’» Repo]()
- [ ] Mixtral (2023): Sparse Mixture of Experts [ğŸ“„ Paper](https://arxiv.org/abs/2401.04088) [ğŸ’» Repo]()
- [ ] Reinforcement Learning from Human Feedback (RLHF) (2022) [ğŸ“„ Paper](https://arxiv.org/abs/2204.05862) [ğŸ’» Repo]()
- [ ] Constitutional AI (CAI) (2023) [ğŸ“„ Paper](https://arxiv.org/abs/2212.08073) [ğŸ’» Repo]()
- [ ] Direct Preference Optimization (DPO) (2023) [ğŸ“„ Paper](https://arxiv.org/abs/2305.18290) [ğŸ’» Repo]()
      
### 5ï¸âƒ£ Image Generative Modeling
- [ ] **Generative Adversarial Networks (2014)** - GAN [ğŸ“„ Paper](https://arxiv.org/abs/1406.2661) [ğŸ’» Repo]()
- [ ] **Auto-Encoding Variational Bayes (2013)** - VAE [ğŸ“„ Paper](https://arxiv.org/abs/1312.6114) [ğŸ’» Repo]()
- [ ] **Deep Convolutional GANs (2015)** - DCGAN [ğŸ“„ Paper](https://arxiv.org/abs/1511.06434) [ğŸ’» Repo]()
- [ ] **CycleGAN (2017)**: Unpaired Image-to-Image Translation [ğŸ“„ Paper](https://arxiv.org/abs/1703.10593) [ğŸ’» Repo]()
- [ ] **Denoising Diffusion Probabilistic Models (2020)** [ğŸ“„ Paper](https://arxiv.org/abs/2006.11239) [ğŸ’» Repo]()
- [ ] **DALL-E (2021)**: Text-to-Image Generation [ğŸ“„ Paper](https://arxiv.org/abs/2102.12092) [ğŸ’» Repo]()
- [ ] **CLIP (2021)**: Visual Models from Natural Language Supervision [ğŸ“„ Paper](https://arxiv.org/abs/2103.00020) [ğŸ’» Repo]()
- [ ] **Rotary Position Embeddings (RoPE) (2021)** [ğŸ“„](https://arxiv.org/abs/2104.09864) [ğŸ’» Repo]()
- [ ] **Grouped-Query Attention (GQA) (2023)** [ğŸ“„](https://arxiv.org/abs/2305.13245) [ğŸ’» Repo]()
- [ ] **Multi-Query Attention (MQA) (2019)** [ğŸ“„](https://arxiv.org/abs/1911.02150) [ğŸ’» Repo]() 
- [ ] **Sliding Window Attention (2020)** [ğŸ“„](https://arxiv.org/abs/2004.05150) [ğŸ’» Repo]()

### 6ï¸âƒ£ Deep Reinforcement Learning
- [ ] **Playing Atari Games (2013)** - Deep Q-Learning [ğŸ“„ Paper](https://arxiv.org/abs/1312.5602) [ğŸ’» Repo]()
- [ ] **Mastering the Game of Go (2016)** - AlphaGo [ğŸ“„ Paper](https://www.nature.com/articles/nature16961) [ğŸ’» Repo]()
- [ ] **Protein Structure Prediction (2021)** - AlphaFold [ğŸ“„ Paper](https://www.nature.com/articles/s41586-021-03819-2) [ğŸ’» Repo]()
- [ ] **Proximal Policy Optimization (2017)** - PPO [ğŸ“„ Paper](https://arxiv.org/abs/1707.06347) [ğŸ’» Repo]()

### 7ï¸âƒ£ General Machine Learning Papers
- [ ] **Random Forests (2001)** - Leo Breiman [ğŸ“„ Paper](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf) [ğŸ’» Repo]()
- [ ] **XGBoost: A Scalable Tree Boosting System (2016)** - Chen and Guestrin [ğŸ“„ Paper](https://arxiv.org/abs/1603.02754) [ğŸ’» Repo]()
- [ ] **Scaling Laws for Neural Language Models (2020)** [ğŸ“„ Paper](https://arxiv.org/abs/2001.08361) [ğŸ’» Repo]()
- [ ] **DistilBERT (2019)**: Model Compression [ğŸ“„ Paper](https://arxiv.org/abs/1910.01108) [ğŸ’» Repo]()

### 8ï¸âƒ£ CUDA & Triton Optimization Papers
- [ ] **FlashAttention (2022)**: Fast and Memory-Efficient Attention [ğŸ“„ Paper](https://arxiv.org/abs/2205.14135) [ğŸ’» Repo]()
- [ ] **FlashAttention-2 (2023)**: Faster Attention Mechanism [ğŸ“„ Paper](https://arxiv.org/abs/2307.08691) [ğŸ’» Repo]()
- [ ] **Triton Compiler (2022)**: Custom GPU Kernels [ğŸ“„ Paper](https://arxiv.org/abs/2203.03466) [ğŸ’» Repo]()

### 9ï¸âƒ£ Generative AI (GenAI)
- [ ] **InstructGPT (2022)**: Following Instructions with Human Feedback [ğŸ“„ Paper](https://arxiv.org/abs/2203.02155) [ğŸ’» Repo]()
- [ ] **LoRA (2021)**: Low-Rank Adaptation of Large Language Models [ğŸ“„ Paper](https://arxiv.org/abs/2106.09685) [ğŸ’» Repo]()
- [ ] **Direct Preference Optimization (DPO) (2023)** [ğŸ“„ Paper](https://arxiv.org/abs/2305.18290) [ğŸ’» Repo]()
- [ ] **DeepSeek LLM (2024)**: Scaling Open-Source Language Models [ğŸ“„ Paper](https://arxiv.org/abs/2401.02954) [ğŸ’» Repo]()

### ğŸ”Ÿ Scaling & Model Optimization
- [ ] **Chinchilla (2022)**: Training Compute-Optimal Large Language Models [ğŸ“„ Paper](https://arxiv.org/abs/2203.15556) [ğŸ’» Repo]()
- [ ] **Switch Transformers (2022)**: Scaling to Trillion Parameter Models [ğŸ“„ Paper](https://arxiv.org/abs/2101.03961) [ğŸ’» Repo]()
- [ ] **GLaM (2021)**: Efficient Scaling with Mixture of Experts [ğŸ“„ Paper](https://arxiv.org/abs/2112.06905) [ğŸ’» Repo]()

### 1ï¸âƒ£1ï¸âƒ£ Reasoning & Capabilities
- [ ] **Chain of Thought Prompting (2022)**: Reasoning with Language Models [ğŸ“„ Paper](https://arxiv.org/abs/2201.11903) [ğŸ’» Repo]()
- [ ] **Tree of Thoughts (2023)**: Deliberate Problem Solving [ğŸ“„ Paper](https://arxiv.org/abs/2305.10601) [ğŸ’» Repo]()

### 1ï¸âƒ£2ï¸âƒ£ Inference & Efficiency Techniques
- [ ] **QLoRA (2023)**: Efficient Fine-Tuning of Quantized Models [ğŸ“„ Paper](https://arxiv.org/abs/2305.14314) [ğŸ’» Repo]()
- [ ] **Knowledge Distillation (2022)**: Comprehensive Survey [ğŸ“„ Paper](https://arxiv.org/abs/2006.05525) [ğŸ’» Repo]()
- [ ] **Pruning and Quantization Techniques (2022)**: Model Compression Survey [ğŸ“„ Paper](https://arxiv.org/abs/2103.00263) [ğŸ’» Repo]()

### 1ï¸âƒ£3ï¸âƒ£ Fine-tuning & Adaptation
- [ ] **P-Tuning (2021)**: Prompt Tuning with Soft Prompts [ğŸ“„ Paper](https://arxiv.org/abs/2103.10385) [ğŸ’» Repo]()
- [ ] **Prefix-Tuning (2021)**: Optimizing Continuous Prompts [ğŸ“„ Paper](https://arxiv.org/abs/2101.00190) [ğŸ’» Repo]()
- [ ] **AdaLoRA (2023)**: Adaptive Low-Rank Adaptation [ğŸ“„ Paper](https://arxiv.org/abs/2303.10512) [ğŸ’» Repo]()

### 1ï¸âƒ£4ï¸âƒ£ Graph Neural Networks
- [ ] **Graph Convolutional Networks (2016)** - GCN [ğŸ“„ Paper](https://arxiv.org/abs/1609.02907) [ğŸ’» Repo]()
- [ ] **Graph Attention Networks (2018)** - GAT [ğŸ“„ Paper](https://arxiv.org/abs/1710.10903) [ğŸ’» Repo]()

### 1ï¸âƒ£5ï¸âƒ£ Self-Supervised and Few-Shot Learning
- [ ] **Bootstrap Your Own Latent (2020)** - BYOL [ğŸ“„ Paper](https://arxiv.org/abs/2006.07733) [ğŸ’» Repo]()
- [ ] **Prototypical Networks (2017)** [ğŸ“„ Paper](https://arxiv.org/abs/1703.05175) [ğŸ’» Repo]()

---

## ğŸ›  Implementation Guidelines
1. **Start Simple**: Begin by implementing models with easy-to-use libraries like Scikit-learn or TensorFlow.
2. **Replicate Results**: Try to reproduce the results from the paper before adding your tweaks.
3. **Analyze**: Evaluate your implementation with proper metrics and compare with the paper's results.
4. **Visualize**: Use plots to understand model performance and feature importance.

---

## ğŸ“¥ How to Use
1. Navigate to the relevant category in the repository.
2. Read the research papers and access the implementations.
3. Check the boxes as you complete each implementation.
4. Experiment with implementations to gain deeper insights into each concept.

---

## ğŸ”— References & Credits
Inspired by **[@saurabhaloneai](https://github.com/saurabhaloneai/History-of-Deep-Learning.git)** and various research papers from **ArXiv, NeurIPS, CVPR, and ICML**.

## ğŸ§  Maintained by
Maintained by **Abinash Pradhan** - [@abinashpradhan01](https://github.com/abinashpradhan01).
